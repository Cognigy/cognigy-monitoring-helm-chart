{{- $targetNamespace := .Values.appNamespacesTarget }}
{{- if .Values.rules.ai.enabled -}}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ .Release.Name }}-cognigy-ai-rules
  labels:
    {{- include "cognigy-monitors.labels" . | nindent 4 }}
spec:
  groups:
  - name: cognigy.alert.host
    rules:
      - alert: HostUnusualNetworkThroughputIn
        expr: sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Host unusual network throughput in (instance {{`{{`}} $labels.instance }})
          description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"

      - alert: HostUnusualNetworkThroughputOut
        expr: sum by (instance) (rate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Host unusual network throughput out (instance {{`{{`}} $labels.instance }})
          description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"

      - alert: HostHighCpuLoad
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Host high CPU load (instance {{`{{`}} $labels.instance }})
          description: "CPU load is > 80%\n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"

      - alert: HostCpuStealNoisyNeighbor
        expr: avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host CPU steal noisy neighbor (instance {{`{{`}} $labels.instance }})
          description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit.\n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"

  - name: cognigy.alert.k8s
    rules:
      - alert: KubeDeploymentReplicasMismatch
        annotations:
          description: Deployment {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.deployment {{`}}`}} has not matched the expected number of replicas for longer than 15 minutes.
          runbook_url: {{ .Values.runbookUrl }}alert-name-kubedeploymentreplicasmismatch
          summary: Deployment has not matched the expected number of replicas.
        expr: |-
          (
            kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
              >
            kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          ) and (
            changes(kube_deployment_status_replicas_updated{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}[10m])
              ==
            0
          )
        for: 15m
        labels:
          severity: critical

      - alert: KubeStatefulSetReplicasMismatch
        annotations:
          description: StatefulSet {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.statefulset {{`}}`}} has not matched the expected number of replicas for longer than 15 minutes.
          runbook_url: {{ .Values.runbookUrl }}alert-name-kubestatefulsetreplicasmismatch
          summary: Deployment has not matched the expected number of replicas.
        expr: |-
          (
            kube_statefulset_status_replicas_ready{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
              !=
            kube_statefulset_status_replicas{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}
          ) and (
            changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}[10m])
              ==
            0
          )
        for: 15m
        labels:
          severity: critical

      - alert: KubePodCrashLooping
        annotations:
          description: Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} ({{`{{`}} $labels.container {{`}}`}}) is restarting {{`{{`}} printf "%.2f" $value {{`}}`}} times / 10 minutes.
          runbook_url: {{ .Values.runbookUrl }}alert-name-kubepodcrashlooping
          summary: Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} is crash looping.
        expr: >
          increase(kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"}[10m]) > 0
          and
          kube_pod_container_status_waiting{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}"} == 1
        for: 15m
        labels:
          severity: critical

      - alert: KubePodNotReady
        annotations:
          description: Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} has been in a non-ready state for longer than 15 minutes.
          runbook_url: {{ .Values.runbookUrl }}alert-name-kubepodnotready
          summary: Pod has been in a non-ready state for more than 15 minutes.
        expr: |-
          sum by (namespace, pod) (
            max by(namespace, pod) (
              kube_pod_status_phase{job="kube-state-metrics", namespace=~"{{ $targetNamespace }}", phase=~"Pending|Unknown"}
            ) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (
              1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})
            )
          ) > 0
        for: 15m
        labels:
          severity: critical

      {{- if not (.Values.rules.ai.disabled.PodManyRestarts | default false ) }}
      - alert: PodManyRestarts
        expr: increase(kube_pod_container_status_restarts_total[60m]) > 3
        annotations:
          summary: Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} restarts too often.
          description: "Pod {{`{{`}} $labels.pod }} restarts in the last 60 minutes \n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"
          runbook_url: {{ .Values.runbookUrl }}alert-name-kubepodcrashlooping
        for: 2m
        labels:
          severity: warning
      {{- end }}

      {{- if not (.Values.rules.ai.disabled.ContainerOOMKilled | default false ) }}
      - alert: ContainerOOMKilled
        expr: |-
          (sum (kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}) by (container)) -
          (sum (kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} offset 5m) by (container)) > 0
        annotations:
          summary: Container {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.container {{`}}`}} OOM Killed
          description: "Container {{`{{`}} $labels.container }} OOMKilled in namespace {{`{{`}} $labels.namespace {{`}}`}}\n LABELS = {{`{{`}} $labels }}"
        for: 3m
        labels:
          severity: warning
      {{- end }}

      {{- if not (.Values.rules.ai.disabled.CPUThrottlingBackendServiceHigh | default false ) }}
      - alert: CPUThrottlingBackendServiceHigh
        annotations:
          description: '{{`{{`}} $value | humanizePercentage {{`}}`}} throttling of CPU in namespace {{`{{`}} $labels.namespace {{`}}`}} for container {{`{{`}} $labels.container {{`}}`}} in pod {{`{{`}} $labels.pod {{`}}`}}.'
          runbook_url: {{ .Values.runbookUrl }}alert-name-cputhrottlinghigh
          summary: Processes experience elevated CPU throttling.
        expr: |-
          sum(increase(container_cpu_cfs_throttled_periods_total{container!="", pod=~"mongodb.*|redis.*|rabbitmq.*"}[5m])) by (container, pod, namespace)
            /
          sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)
            > ( 25 / 100 )
        for: 15m
        labels:
          severity: warning
      {{- end }}

  {{- if not (.Values.rules.ai.disabled.RabbitMq | default false ) }}
  - name: cognigy.alert.rabbitmq
    rules:
      - alert: RabbitmqMemoryHigh
        expr: rabbitmq_process_resident_memory_bytes / rabbitmq_resident_memory_limit_bytes * 100 > 90
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Rabbitmq memory high (instance {{`{{`}} $labels.instance }})
          description: "A node use more than 90% of allocated RAM\n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"

      - alert: RabbitMQ High watermark limit crossed
        expr: rabbitmq_resident_memory_limit_bytes - rabbitmq_process_resident_memory_bytes < 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: High watermark limit has been crossed
          description: High watermark limit has been crossed

      - alert: RabbitMQ Ready message is getting high
        expr: sum(rate(rabbitmq_queue_messages_ready[15m])) > 5
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: RabbitMQ Ready message is getting high
          description: RabbitMQ Ready message is getting high

      - alert: RabbitmqReadyQueueHigh
        expr: rabbitmq_queue_messages_ready > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: RabbitMQ Ready message is high
          description: RabbitMQ Ready queue has crossed warning high threshold

      - alert: RabbitmqReadyQueueHigh
        expr: rabbitmq_queue_messages_ready > 1000
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: RabbitMQ Ready message is high
          description: RabbitMQ Ready queue has crossed critical high threshold

      - alert: RabbitMQ Unacked message is getting high
        expr: sum(rate(rabbitmq_queue_messages_unacked[15m])) > 10
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: RabbitMQ Unacked message is getting high
          description: RabbitMQ Unacked message is getting high
  {{- end }}

  {{- if not (.Values.rules.ai.disabled.Redis | default false ) }}
  - name: cognigy.alert.redis
    rules:
      - alert: RedisDown
        expr: redis_up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Redis down (instance {{`{{`}} $labels.instance }})
          description: "Redis instance is down\n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"
      - alert: RedisMemoryHigh
        expr: >
          (redis_memory_max_bytes{} > 0) and (redis_memory_used_bytes{} * 100/redis_memory_max_bytes{} > 90)
        for: 2m
        labels:
          severity: error
        annotations:
          summary: Redis instance {{ "{{ $labels.instance }}" }} is using too much memory
          description: |
            Redis instance {{ "{{ $labels.instance }}" }} is using {{ "{{ $value }}" }}% of its available memory.
      - alert: RedisBlockedClients
        expr: increase(redis_blocked_clients[5m]) > 0
        for: 2m
        labels:
          severity: error
        annotations:
          summary: Redis instance {{ "{{ $labels.instance }}" }} has blocked clients
          description: |
            Redis instance {{ "{{ $labels.instance }}" }} has blocked {{ "{{ $value }}" }} clients
      - alert: RedisContainerRestarted
        expr: increase(kube_pod_container_status_restarts_total{container="redis"}[5m]) > 0
        annotations:
          summary: Redis container in pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} has restarted
          description: "Redis container in pod {{`{{`}} $labels.pod }} has restarted\n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"
        for: 2m
        labels:
          severity: warning
  {{- end }}

  {{- if not (.Values.rules.ai.disabled.MongoDb | default false ) }}
  - name: cognigy.alert.mongodb
    rules:
      - alert: MongodbDown
        expr: mongodb_up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: MongoDB Down (instance {{`{{`}} $labels.instance }})
          description: "MongoDB instance is down\n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"
  {{- end }}

  {{- if not (.Values.rules.ai.disabled.NlpMatcher | default false ) }}
  - name: cognigy.alert.nlp-matcher
    rules:
      - alert: MatcherFindKeyphraseError
        expr: cognigy_matcher_findkeyphrases_error > 0
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: NLP matcher had findKeyphrase error (instance {{`{{`}} $labels.instance }})
          description: "NLP Matcher had findKeyphrase errors: \n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"
  {{- end }}

  {{- if not (.Values.rules.ai.disabled.FunctionExecution | default false ) }}
  - name: cognigy.alert.function-execution
    rules:
      - alert: FunctionSchedulerSchedulingConflict
        expr: increase(cognigy_function_scheduler_scheduling_conflict_count[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: functions scheduler scheduling conflicts on (instance {{`{{`}} $labels.instance }})
          description: "Number of scheduling functions per organization has reached the limit. \n VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"
  {{- end }}

  {{- if not (.Values.rules.ai.disabled.ServiceExecution | default false ) }}
  - name: cognigy.alert.service-execution
    rules:
      - alert: ServiceExecutionTimeHigh
        expr: histogram_quantile(0.99, sum(rate(cognigy_execution_extension_processing_time_bucket[5m])) by (le))/1000 > 15
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: extension execution time is higher than 15 sec (instance {{`{{`}} $labels.instance }})
          description: "Extension execution time on service-execution is {{`{{`}} $value }} sec over last 15 minutes\n LABELS = {{`{{`}} $labels }}"
  {{- end }}

  {{- if not (.Values.rules.ai.disabled.ServiceEndpoint | default false ) }}
  - name: cognigy.alert.endpoint
    rules:
      - alert: EndpointMessageProcessingTimeIncreasing
        expr: >
              100 *(histogram_quantile({{ 0.95 }}, sum(rate(cognigy_endpoint_raw_messages_response_time_bucket[5m])) by (le)) -
              histogram_quantile({{ $.Values.rules.ai.ServiceEndpoint.MessageProcessingTimeQuantile }}, sum(rate(cognigy_endpoint_raw_messages_response_time_bucket[5m] offset 15m)) by (le)))/
              histogram_quantile({{ $.Values.rules.ai.ServiceEndpoint.MessageProcessingTimeQuantile }}, sum(rate(cognigy_endpoint_raw_messages_response_time_bucket[5m] offset 15m)) by (le)) >
              {{ $.Values.rules.ai.ServiceEndpoint.MessageProcessingTimeIncreasingThreshold }}
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Messages processing time has increased over {{ $.Values.rules.ai.ServiceEndpoint.MessageProcessingTimeIncreasingThreshold }}% in last 15 minutes on (instance {{`{{`}} $labels.instance }})
          description: "Messages processing time has increased by {{`{{`}} $value }}% in the last 15 minutes\n LABELS = {{`{{`}} $labels }}"
      - alert: EndpointMessageProcessingTimeHigh
        expr: >
              histogram_quantile( {{ $.Values.rules.ai.ServiceEndpoint.MessageProcessingTimeQuantile }},
              sum(rate(cognigy_endpoint_raw_messages_response_time_bucket[5m])) by (le))/1000 >
              {{ $.Values.rules.ai.ServiceEndpoint.MessageProcessingTimeThreshold }}
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Messages processing is higher than {{ $.Values.rules.ai.ServiceEndpoint.MessageProcessingTimeThreshold }} sec on (instance {{`{{`}} $labels.instance }})
          description: "Messages processing time is {{`{{`}} $value }} sec over last 15 minutes\n LABELS = {{`{{`}} $labels }}"
  {{- end }}

  {{- if not (.Values.rules.ai.disabled.ServiceHandover | default false ) }}
  - name: cognigy.alert.service-handover
    rules:
      - alert: ServiceHandoverErrorsHigh
        expr: sum (increase(cognigy_ai_handover_error_amount[5m])) by (projectId) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: handover errors in (projectId {{`{{`}} $labels.projectId }}) is high
          description: "Number of handover errors is {{`{{`}} $value }} in last 5 minutes\n LABELS = {{`{{`}} $labels }}"
  {{- end }}

{{- end -}}