{{- $targetNamespace := .Values.appNamespacesTarget }}
{{- if .Values.rules.ai.enabled -}}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ .Release.Name }}-cognigy-ai-rules
  labels:
    {{- include "cognigy-monitors.labels" . | nindent 4 }}
spec:
  groups:
  {{- if (.Values.rules.ai.cpuThrottlingAIServiceHigh.enabled ) }}
  - name: cognigy.ai.alert.general
    rules:
    - alert: CPUThrottlingAIServiceHigh
      annotations:
        description: '{{`{{`}} $value | humanizePercentage {{`}}`}} throttling of CPU in namespace {{`{{`}} $labels.namespace {{`}}`}} for container {{`{{`}} $labels.container {{`}}`}} in pod {{`{{`}} $labels.pod {{`}}`}}.'
        runbook_url: {{ .Values.runbookUrl }}alert-name-cputhrottlinghigh
        summary: Processes experience elevated CPU throttling.
      expr: |-
        sum(increase(container_cpu_cfs_throttled_periods_total{container=~"^service-endpoint|service-ai"}[5m])) by (container, pod, namespace)
          /
        sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)
          > ( 25 / 100 )
      for: {{ .Values.rules.ai.cpuThrottlingAIServiceHigh.timeout }}
      labels:
        severity: {{ .Values.rules.ai.cpuThrottlingAIServiceHigh.severity }}
  {{- end }}
  {{- if or (.Values.rules.ai.conversationVolume.enabled ) (.Values.rules.ai.conversationVolumeDynamic.enabled ) }}
  - name: cognigy.alert.conversation-volume
    rules:
      {{- if (.Values.rules.ai.conversationVolume.enabled ) }}
      - alert: highConversationVolume
        expr: sum(increase(cognigy_endpoint_raw_messages_count[1m])) >
          {{$.Values.rules.ai.conversationVolume.threshold }}
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: Messages/min rate is high (instance {{`{{`}} $labels.instance }})
          description: "Messages/min rate has crossed {{$.Values.rules.ai.conversationVolume.threshold }} threshold: \n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"
      {{- end }}
      {{- if (.Values.rules.ai.conversationVolumeDynamic.enabled ) }}
      - alert: highConversationVolumeDynamic
        expr: >
          sum(increase(cognigy_endpoint_raw_messages_count[1m])) > 
          (1 + {{$.Values.rules.ai.conversationVolumeDynamic.percentageThreshold}}/100) * 
          quantile_over_time(0.95, sum(increase(cognigy_endpoint_raw_messages_count[1m]))[{{$.Values.rules.ai.conversationVolumeDynamic.baselineDays}}d:1m])
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Messages/min rate is {{$.Values.rules.ai.conversationVolumeDynamic.percentageThreshold}}% higher than peak levels (instance {{`{{`}} $labels.instance }})
          description: "Current messages/min rate ({{`{{`}} $value | printf \"%.0f\" }}) is {{$.Values.rules.ai.conversationVolumeDynamic.percentageThreshold}}% higher than the 95th percentile of the past {{$.Values.rules.ai.conversationVolumeDynamic.baselineDays}} days\n  LABELS = {{`{{`}} $labels }}"
      {{- end }}
  {{- end }}

  {{- if (.Values.rules.ai.nlpMatcher.enabled ) }}
  - name: cognigy.alert.nlp-matcher
    rules:
      - alert: MatcherFindKeyphraseError
        expr: increase(cognigy_matcher_findkeyphrases_error[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: NLP matcher had findKeyphrase error (instance {{`{{`}} $labels.instance }})
          description: "NLP Matcher had findKeyphrase errors: \n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"
  {{- end }}

  {{- if (.Values.rules.ai.functionExecution.enabled ) }}
  - name: cognigy.alert.function-execution
    rules:
      - alert: FunctionSchedulerSchedulingConflict
        expr: increase(cognigy_function_scheduler_scheduling_conflict_count[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: functions scheduler scheduling conflicts on (instance {{`{{`}} $labels.instance }})
          description: "Number of scheduling functions per organization has reached the limit. \n VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"
  {{- end }}

  {{- if (.Values.rules.ai.serviceExecution.enabled ) }}
  - name: cognigy.alert.service-execution
    rules:
      - alert: ServiceExecutionTimeHigh
        expr: histogram_quantile(0.99, sum(rate(cognigy_execution_extension_processing_time_bucket[5m])) by (le))/1000 > 15
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: extension execution time is higher than 15 sec (instance {{`{{`}} $labels.instance }})
          description: "Extension execution time on service-execution is {{`{{`}} $value }} sec over last 15 minutes\n LABELS = {{`{{`}} $labels }}"
  {{- end }}

  {{- if (.Values.rules.ai.serviceEndpoint.enabled ) }}
  - name: cognigy.alert.endpoint
    rules:
      - alert: EndpointMessageProcessingTimeIncreasing
        expr: >
          (sum(rate(cognigy_endpoint_raw_messages_response_time_sum[180m]))/sum(rate(cognigy_endpoint_raw_messages_response_time_count[180m]))/1000 >
          {{ $.Values.rules.ai.serviceEndpoint.messageProcessingTimeBaseline }}) and
          (100*((sum(rate(cognigy_endpoint_raw_messages_response_time_sum[15m]))/sum(rate(cognigy_endpoint_raw_messages_response_time_count[15m])))
          - (sum(rate(cognigy_endpoint_raw_messages_response_time_sum[180m]))/sum(rate(cognigy_endpoint_raw_messages_response_time_count[180m]))))/
          (sum(rate(cognigy_endpoint_raw_messages_response_time_sum[180m]))/sum(rate(cognigy_endpoint_raw_messages_response_time_count[180m]))) >
          {{ $.Values.rules.ai.serviceEndpoint.messageProcessingTimeIncreasingThreshold }})
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Average message processing time has increased over {{ $.Values.rules.ai.serviceEndpoint.messageProcessingTimeIncreasingThreshold }}% in last 15 minutes compared to previous 3 hours (instance {{`{{`}} $labels.instance }})
          description: "Average message processing time has increased by {{`{{`}} $value }}% in the last 15 minutes\n LABELS = {{`{{`}} $labels }}"
      - alert: EndpointMessageProcessingTimeHigh
        expr: >
              histogram_quantile( {{ $.Values.rules.ai.serviceEndpoint.messageProcessingTimeQuantile }},
              sum(rate(cognigy_endpoint_raw_messages_response_time_bucket[5m])) by (le))/1000 >
              {{ $.Values.rules.ai.serviceEndpoint.messageProcessingTimeThreshold }}
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Message processing time is higher than {{ $.Values.rules.ai.serviceEndpoint.messageProcessingTimeThreshold }} sec for {{ $.Values.rules.ai.serviceEndpoint.messageProcessingTimeQuantile }}% of messages on (instance {{`{{`}} $labels.instance }})
          description: "Message processing time is {{`{{`}} $value }} sec for {{ $.Values.rules.ai.serviceEndpoint.messageProcessingTimeQuantile }}% of messages for last 15 minutes\n LABELS = {{`{{`}} $labels }}"
      {{- if (.Values.rules.ai.serviceEndpoint.serviceEndpointOOMKilled.enabled ) }}
      - alert: ServiceEndpointOOMKilled
        expr: |-
          (sum by (container) (kube_pod_container_status_last_terminated_reason{reason="OOMKilled", container=~"^service-endpoint"})) >
          {{ .Values.rules.ai.serviceEndpoint.serviceEndpointOOMKilled.threshold }}
          UNLESS ((sum by (container, cluster) (kube_pod_container_status_last_terminated_reason{reason="OOMKilled", container=~"^service-endpoint"})) -
          (sum by (container) (kube_pod_container_status_last_terminated_reason{reason="OOMKilled", container=~"^service-endpoint"}
          offset {{ .Values.rules.ai.serviceEndpoint.serviceEndpointOOMKilled.offset }})) == 0)
        annotations:
          summary: Container service-endpoint OOMKilled {{`{{`}} $value {{`}}`}} times
          description: "Container service-endpoint OOMKilled {{`{{`}} $value }} times \n LABELS = {{`{{`}} $labels }}"
        for: {{ .Values.rules.ai.serviceEndpoint.serviceEndpointOOMKilled.timeout }}
        labels:
          severity: {{ .Values.rules.ai.serviceEndpoint.serviceEndpointOOMKilled.severity }}
      {{- end }}
  {{- end }}

  {{- if (.Values.rules.ai.serviceHandover.enabled ) }}
  - name: cognigy.alert.service-handover
    rules:
      - alert: ServiceHandoverErrorsHigh
        expr: sum (increase(cognigy_ai_handover_error_amount[5m])) by (organisationId, projectId) >  {{ $.Values.rules.ai.serviceHandover.threshold }}
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: handover errors in organisationId {{`{{`}} $labels.organisationId }} projectId {{`{{`}} $labels.projectId }} is high
          description: "Number of handover errors is {{`{{`}} $value }} in last 5 minutes\n LABELS = {{`{{`}} $labels }}"
      - alert: ServiceHandoverRequestLatencyHigh
        expr: >
          histogram_quantile( {{ $.Values.rules.ai.serviceHandover.requestLatencyHighQuantile }},
          sum(rate(cognigy_http_request_duration_bucket{container="service-handover"}[1m])) by (le, route))/1000 >
              {{ $.Values.rules.ai.serviceHandover.requestLatencyHighThreshold }}
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: service-handover HTTP requests latency for provider {{`{{`}} $labels.route }} is high
          description: "service-handover HTTP requests latency is higher than {{`{{`}} $value }} sec\n LABELS = {{`{{`}} $labels }}"
      - alert: ServiceHandoverRequest5xxErrorRate
        expr: >
          sum by (route) (increase(cognigy_http_request_duration_count{container="service-handover", status=~"5.*"}[1m])) >
            {{ $.Values.rules.ai.serviceHandover.requestErrorThreshold }}
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: service-handover 5xx HTTP requests rate for provider {{`{{`}} $labels.route }} is high
          description: "service-handover 5xx HTTP requests rate for is {{`{{`}} $value }} requests/min\n LABELS = {{`{{`}} $labels }}"
 
      - alert: ServiceHandoverRPCCallFailureRateHigh
        expr: |
          (
            sum by (providerId, organisationId) (increase(cognigy_ai_handover_request_handover_rpc_calls_error_count{providerId!="none", providerId!="salesforce"}[{{ $.Values.rules.ai.serviceHandover.requestHandoverRPCCallFailureRate.evaluationWindow }}]))
          /
            clamp_min(sum by (providerId, organisationId) (increase(cognigy_ai_handover_request_handover_rpc_calls_total{providerId!="none", providerId!="salesforce"}[{{ $.Values.rules.ai.serviceHandover.requestHandoverRPCCallFailureRate.evaluationWindow }}])), 1)
          ) * 100 > {{ $.Values.rules.ai.serviceHandover.requestHandoverRPCCallFailureRate.threshold }}
          and
          sum by (providerId, organisationId) (increase(cognigy_ai_handover_request_handover_rpc_calls_error_count{providerId!="none", providerId!="salesforce"}[{{ $.Values.rules.ai.serviceHandover.requestHandoverRPCCallFailureRate.evaluationWindow }}])) > {{ $.Values.rules.ai.serviceHandover.requestHandoverRPCCallFailureRate.numberOfErrors }}
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'Provider {{ "{{" }} $labels.providerId {{ "}}" }} RPC failure rate is high ({{ "{{" }} $value | printf "%.0f" {{ "}}" }}%)'
          description: 'Provider: {{ "{{" }} $labels.providerId {{ "}}" }}, Org: {{ "{{" }} $labels.organisationId {{ "}}" }} - Current failure rate: {{ "{{" }} $value | printf "%.2f" {{ "}}" }}% (threshold: {{ $.Values.rules.ai.serviceHandover.requestHandoverRPCCallFailureRate.threshold }}%)'

      - alert: ServiceHandoverSendMessageToProviderErrorRateHigh
        expr: |
          (
            sum by (providerId, organisationId) (increase(cognigy_ai_handover_send_message_to_provider_rpc_error_count{providerId!="none"}[{{ $.Values.rules.ai.serviceHandover.sendMessageToProviderErrorRate.evaluationWindow }}]))
          /
            clamp_min(sum by (providerId, organisationId) (increase(cognigy_ai_handover_send_message_to_provider_rpc_calls_total{providerId!="none"}[{{ $.Values.rules.ai.serviceHandover.sendMessageToProviderErrorRate.evaluationWindow }}])), 1) * 100
          )
          > {{ $.Values.rules.ai.serviceHandover.sendMessageToProviderErrorRate.threshold }}
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: 'Provider {{ "{{" }} $labels.providerId {{ "}}" }} SendMessageToProvider RPC error rate is high ({{ "{{" }} $value | printf "%.0f" {{ "}}" }}%)'
          description: 'Provider: {{ "{{" }} $labels.providerId {{ "}}" }}, Org: {{ "{{" }} $labels.organisationId {{ "}}" }} - Current error rate: {{ "{{" }} $value | printf "%.2f" {{ "}}" }}% (threshold: {{ $.Values.rules.ai.serviceHandover.sendMessageToProviderErrorRate.threshold }}%)'

      - alert: ServiceHandoverEventRPCErrorRateHigh
        expr: |
          (
            sum by (providerId, organisationId) (increase(cognigy_ai_handover_rpc_event_error_count{providerId!="none"}[{{ $.Values.rules.ai.serviceHandover.eventRpcErrorRate.evaluationWindow }}]))
          /
            clamp_min(sum by (providerId, organisationId) (increase(cognigy_ai_handover_rpc_event_rpc_calls{providerId!="none"}[{{ $.Values.rules.ai.serviceHandover.eventRpcErrorRate.evaluationWindow }}])), 1)
          ) * 100 > {{ $.Values.rules.ai.serviceHandover.eventRpcErrorRate.threshold }}
          and
          sum by (providerId, organisationId) (increase(cognigy_ai_handover_rpc_event_error_count{providerId!="none"}[{{ $.Values.rules.ai.serviceHandover.eventRpcErrorRate.evaluationWindow }}])) > {{ $.Values.rules.ai.serviceHandover.eventRpcErrorRate.numberOfErrors }}
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: 'Provider {{ "{{" }} $labels.providerId {{ "}}" }} Event RPC error rate is high ({{ "{{" }} $value | printf "%.0f" {{ "}}" }}%)'
          description: 'Provider: {{ "{{" }} $labels.providerId {{ "}}" }}, Org: {{ "{{" }} $labels.organisationId {{ "}}" }} - Current error rate: {{ "{{" }} $value | printf "%.2f" {{ "}}" }}% (threshold: {{ $.Values.rules.ai.serviceHandover.eventRpcErrorRate.threshold }}%)'

      - alert: ServiceHandoverContainerRestartWarning
        expr: increase(kube_pod_container_status_restarts_total{container=~"service-handover"}[{{ $.Values.rules.ai.serviceHandover.containerRestart.evaluationWindow }}]) > {{ $.Values.rules.ai.serviceHandover.containerRestart.threshold }}
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: 'Container {{ "{{" }} $labels.container {{ "}}" }} in pod {{ "{{" }} $labels.pod {{ "}}" }} (namespace {{ "{{" }} $labels.namespace {{ "}}" }}) has restarted several times in the last {{ $.Values.rules.ai.serviceHandover.containerRestart.evaluationWindow }}.'
          description: 'The container {{ "{{" }} $labels.container {{ "}}" }} (pod: {{ "{{" }} $labels.pod {{ "}}" }}, namespace: {{ "{{" }} $labels.namespace {{ "}}" }}) has recorded {{ "{{" }} $value | printf "%.0f" {{ "}}" }} restarts in the last {{ $.Values.rules.ai.serviceHandover.containerRestart.evaluationWindow }}, indicating potential instability.'

      - alert: ServiceHandoverCriticalSendMessageToProviderErrorRateHigh
        expr: |
          (
            sum by (providerId, organisationId) (increase(cognigy_ai_handover_send_message_to_provider_rpc_error_count{providerId!="none"}[{{ $.Values.rules.ai.serviceHandover.sendMessageToProviderErrorRate.evaluationWindow }}]))
          /
            clamp_min(sum by (providerId, organisationId) (increase(cognigy_ai_handover_send_message_to_provider_rpc_calls_total{providerId!="none"}[{{ $.Values.rules.ai.serviceHandover.sendMessageToProviderErrorRate.evaluationWindow }}])), 1) * 100 
          )
          > {{ $.Values.rules.ai.serviceHandover.sendMessageToProviderErrorRate.criticalThreshold }}
        for: 20m
        labels:
          severity: critical
        annotations:
          summary: 'Provider {{ "{{" }} $labels.providerId {{ "}}" }} SendMessageToProvider RPC error rate is high ({{ "{{" }} $value | printf "%.0f" {{ "}}" }}%)'
          description: 'Provider: {{ "{{" }} $labels.providerId {{ "}}" }}, Org: {{ "{{" }} $labels.organisationId {{ "}}" }} - Current error rate: {{ "{{" }} $value | printf "%.2f" {{ "}}" }}% (threshold: {{ $.Values.rules.ai.serviceHandover.sendMessageToProviderErrorRate.criticalThreshold }}%)'

  {{- end }}

  {{- if (.Values.rules.ai.serviceHttp.enabled ) }}
  - name: cognigy.alert.service-http
    rules:
      - alert: ServiceHttpRequestLatencyHigh
        expr: >
          histogram_quantile( {{ $.Values.rules.ai.serviceHttp.requestLatencyHighQuantile }},
            sum(increase(cognigy_service_http_external_http_requests_latency_ms_bucket[1m])) by (le, orgId, projectId))/1000 >
            {{ $.Values.rules.ai.serviceHttp.requestLatencyHighThreshold }}
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: service-HTTP requests latency is high for orgId {{`{{`}} $labels.orgId  }} projectId {{`{{`}} $labels.projectId  }}
          description: "Service-HTTP requests latency is higher than {{`{{`}} $value }} sec\n LABELS = {{`{{`}} $labels }}"
  {{- end }}

  {{- if (.Values.rules.ai.serviceCollaboration.enabled ) }}
  # Service Collaboration
  - name: cognigy.alert.service-collaboration
    rules:
    - alert: UncaughtErrorsServiceCollaboration
      expr: sum(increase(cognigy_collaboration_uncaught_error_total[5m])) + sum(increase(cognigy_collaboration_unhandled_rejection_total[5m])) > {{ $.Values.rules.ai.serviceCollaboration.unhandledErrorThreshold }}
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: service-collaboration had uncaught errors on (instance {{`{{`}} $labels.instance }})
        description: "service-collaboration had uncaught errors: \n  VALUE = {{`{{`}} $value }}\n  LABELS = {{`{{`}} $labels }}"
    - alert: ServiceCollaborationRequestLatencyHigh
      expr: histogram_quantile(0.99, sum by (le) (increase(cognigy_collaboration_create_thread_execution_time_bucket[5m])+increase(cognigy_collaboration_index_threads_execution_time_bucket[5m])+increase(cognigy_collaboration_join_activity_execution_time_bucket[5m])+increase(cognigy_collaboration_join_cursor_tracking_execution_time_bucket[5m])+increase(cognigy_collaboration_leave_activity_execution_time_bucket[5m])+increase(cognigy_collaboration_leave_cursor_tracking_execution_time_bucket[5m])+increase(cognigy_collaboration_subscribe_to_threads_execution_time_bucket[5m])+increase(cognigy_collaboration_unsubscribe_from_threads_execution_time_bucket[5m])+increase(cognigy_collaboration_update_activity_execution_time_bucket[5m])+increase(cognigy_collaboration_update_cursor_tracking_execution_time_bucket[5m])+increase(cognigy_collaboration_create_thread_message_execution_time_bucket[5m])+increase(cognigy_collaboration_delete_thread_execution_time_bucket[5m])+increase(cognigy_collaboration_delete_thread_message_execution_time_bucket[5m])+increase(cognigy_collaboration_read_thread_execution_time_bucket[5m])+increase(cognigy_collaboration_update_thread_execution_time_bucket[5m])+increase(cognigy_collaboration_update_thread_message_execution_time_bucket[5m]))) > {{ $.Values.rules.ai.serviceCollaboration.requestLatencyHighThreshold }}
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: collaboration request latency is higher than 500ms sec (instance {{`{{`}} $labels.instance }})
        description: "collaboration request latency is {{`{{`}} $value }} sec over last 5 minutes\n LABELS = {{`{{`}} $labels }}"
  {{- end }}
{{- end -}}